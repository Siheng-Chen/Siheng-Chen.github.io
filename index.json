[{"authors":null,"categories":null,"content":" ","date":1650370064,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650370064,"objectID":"679c38765274c90859054a929b172e60","permalink":"https://Siheng-Chen.github.io/author/siheng-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/siheng-chen/","section":"authors","summary":"","tags":null,"title":"Siheng Chen","type":"authors"},{"authors":["Chenxin Xu"],"categories":null,"content":" ","date":1650281659,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650281659,"objectID":"e30096c5b4228ad79445938f5d05a5a0","permalink":"https://Siheng-Chen.github.io/author/chenxin-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chenxin-xu/","section":"authors","summary":"","tags":null,"title":"Chenxin Xu","type":"authors"},{"authors":["Maosen Li"],"categories":null,"content":"","date":1650281659,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650281659,"objectID":"5cd85b1f0ee94609b9ce55a71fbad57b","permalink":"https://Siheng-Chen.github.io/author/maosen-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/maosen-li/","section":"authors","summary":"","tags":null,"title":"Maosen Li","type":"authors"},{"authors":["Weibo Mao"],"categories":null,"content":" ","date":1650281659,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650281659,"objectID":"1dfbd7c9c94916841ae52c7cf9b4be09","permalink":"https://Siheng-Chen.github.io/author/weibo-mao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/weibo-mao/","section":"authors","summary":"","tags":null,"title":"Weibo Mao","type":"authors"},{"authors":["Wenjun Zhang"],"categories":null,"content":" ","date":1650281659,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650281659,"objectID":"2bfface6e22cad81a10d8b33105752e6","permalink":"https://Siheng-Chen.github.io/author/wenjun-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wenjun-zhang/","section":"authors","summary":"","tags":null,"title":"Wenjun Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1650281659,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650281659,"objectID":"b32c28b03316fb2e22f3b13269ac5f4b","permalink":"https://Siheng-Chen.github.io/author/ya-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ya-zhang/","section":"authors","summary":"","tags":null,"title":"Ya Zhang","type":"authors"},{"authors":["Zhenyang Ni"],"categories":null,"content":"","date":1650281659,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650281659,"objectID":"7a66e2b56e3d48318b1b6f759cbaf28e","permalink":"https://Siheng-Chen.github.io/author/zhenyang-ni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhenyang-ni/","section":"authors","summary":"","tags":null,"title":"Zhenyang Ni","type":"authors"},{"authors":["Shaoheng Fang","Yue Hu","Weitao Wu","Siheng Chen"],"categories":[],"content":" Table of Contents About Coperception-UAV Dataset Simulation Setting Swarm arrangement Sensor Setup Data Usage Citation About Coperception-UAV Dataset Coperception-UAV is the first comprehensive dataset for UAV-based collaborative perception.\nA UAV swarm has the potential to distribute tasks and achieve better, faster, and more robust performances than a single UAV. To realize this, we need to integrate collaboration ability into the entire pipeline, including perception, planning, control. Among those tasks, collaborative perception enables holistic scene understanding from multiple perspectives via the collaboration of multiple UAVs, which could fundamentally resolve the occlusion issue and the long-range issue in the traditional single-agent perception. Recently, planning and control of a UAV swarm have been intensively studied; however, the collaborative perception remains under-explored due to the lack of a comprehensive dataset. This work aims to fill this gap and proposes a collaborative perception dataset for UAV swarm.\nBased on the co-simulation platform of AirSim and CARLA, our dataset consists of 131.9k synchronous images collected from 5 coordinated UAVs flying at 3 altitudes over 3 simulated towns with 2 swarm formations. Each image is fully annotated with the pixel-wise semantic segmentation labels and 2D/3D bounding boxes of vehicles. We further build a benchmark on the proposed dataset by evaluating a variety of related multi-agent collaborative methods on multiple perception tasks, including object detection, semantic segmentation, and bird’seye-view (BEV) semantic segmentation.\nSimulation Setting Our proposed dataset is collected by the co-simulation of CARLA and AirSim. We use CARLA to generate complex simulation scenes and traffic flow; and use AirSim to simulate UAV swarm flying in the scene. The flight route of UAVs is controlled by AirSim and sample data are collected randomly at about 4-second intervals.\nSwarm arrangement The UAV swarm moves and executes tasks in the three-dimensional space, where the situation could be much more complex than those of vehicles or roadside units. In the dataset, two main factors are taken into consideration that may affect the perception and collaboration patterns of UAV swarms: flight formation and altitude. Each UAV swarm consists of 5 UAVs. We arrange two types of formation modes for a UAV swarm: discipline mode, where all 5 UAVs keeps a consistent and relatively static array, and dynamic mode, where each UAV navigates independently in the scene. The former simulates the situation where the swarm of UAVs is executing a same specific task such as exploring an unknown area, search and rescue; while the latter simulates the monitoring and patrolling tasks in the city.\nSensor Setup In the UAV swarm, Each UAV is equipped with 5 RGB cameras in 5 directions and 5 semantic cameras collecting semantic ground truth for RGB cameras.\n90° horizontal FoV 1 bird’s eye view camera and 4 cameras facing forward, backward, right, and left with a pitch degree of −45° image size: 800x450 pixels Data Fully-annotated data are provided in the dataset, including synchronous images with pixel-wise semantic labels, 2D \u0026amp; 3D bounding boxes of vehicles, and BEV semantic map.\nCamera data We collect synchronous images from all cameras on 5 UAVs, which is 25 images in a sample. In total, 123.8K images are collected for the discipline swarm mode and 8.1K for the dynamic swarm mode. We provide semantic label for each image.\nBounding boxes 3D bounding boxes of vehicles are recorded at the same moment with images, including location (x, y, z), rotation (w, x, y, z in quaternion) in the global coordinate and their length, width and height. To specifically address the occlusion issue, we also provide a binary label for the occlusion status of each bounding box.\nBEV semantic label We provide BEV segmentation labels of four categories: roadway, building, vehicle, and others, which are the key elements to construct the layout of a city and foreground objects. The resolution of the BEV map is 0.25m×0.25m.\nUsage The dataset is organized in a similar way with the widelyused autonomous driving dataset, nuScenes; so it can be used directly with the well-established nuScenes-devkit.\nCitation To Be Done ","date":1650370064,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650370064,"objectID":"97c4d20f8094ee47b07469162a696c84","permalink":"https://Siheng-Chen.github.io/dataset/coperception-uav/","publishdate":"2022-04-19T20:07:44+08:00","relpermalink":"/dataset/coperception-uav/","section":"dataset","summary":"Coperception-UAV is the first comprehensive dataset for UAV-based collaborative perception.","tags":[],"title":"Coperception-UAV Dataset","type":"dataset"},{"authors":["Chenxin Xu","Maosen Li","Zhenyang Ni","Ya Zhang","Siheng Chen"],"categories":["CVPR 2022"],"content":" Abstract Demystifying the interactions among multiple agents from their past trajectories is fundamental to precise and interpretable trajectory prediction. However, previous works only consider pair-wise interactions with limited relational reasoning. To promote more comprehensive interaction modeling for relational reasoning, we propose GroupNet, a multiscale hypergraph neural network, which is novel in terms of both interaction capturing and representation learning. From the aspect of interaction capturing, we propose a trainable multiscale hypergraph to capture both pair-wise and group-wise interactions at multiple group sizes. From the aspect of interaction representation learning, we propose a three-element format that can be learnt end-to-end and explicitly reason some relational factors including the interaction strength and category. We apply GroupNet into both CVAE-based prediction system and previous state-of-the-art prediction systems for predicting socially plausible trajectories with relational reasoning. To validate the ability of relational reasoning, we experiment with synthetic physics simulations to reflect the ability to capture group behaviors, reason interaction strength and interaction category. To validate the effectiveness of prediction, we conduct extensive experiments on three real-world trajectory prediction datasets, including NBA, SDD and ETH-UCY; and we show that with GroupNet, the CVAE-based prediction system outperforms state-of-the-art methods. We also show that adding GroupNet will further improve the performance of previous state-of-the-art prediction systems.\nResult On the relational reasoning Visualization of learnt group behavior. (a) The particle trajectories containing a three-group with a light bar, a two-group with a spring and an individual particle. (b) The heatmap of the learnt affinity matrix via affinity modeling. (c) The multiscale hypergraph topology via hyperedge forming and the interaction category vector of each hyperedge. The red box represents the three-group and two-group we inferred. Particles’ trajectories and the curve of neural interaction strength with particle’s electric charge. We see that the neural interaction strength has a proportional relationship with the amount of charge, reflecting our model is capable to implicitly capture the interaction strength in an unsupervised manner. Visualization results on the NBA dataset. We plot the best trajectory among 20 predictions for the state-of-the-art method (NMMP), GroupNet with the CVAE framework (Ours) and ground truth (GT). The red/blue color represents players of two teams and the green color represents the basketball. Light color represents the past trajectory. Citation @InProceedings{xu2022GroupNet, author = {Xu, Chenxin and Li, Maosen and Ni, Zhenyang and Zhang, Ya and Chen, Siheng}, title = {GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning}, booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year = {2022} } Acknowledgement This research is partially supported by the National Key R\u0026amp;D Program of China under Grant 2021ZD0112801, National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CCF-DiDi GAIA Research Collaboration Plan 202112.\n","date":1650281659,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650281659,"objectID":"3d38088b7f5365713b140862db227f1f","permalink":"https://Siheng-Chen.github.io/project/groupnet/","publishdate":"2022-04-18T19:34:19+08:00","relpermalink":"/project/groupnet/","section":"project","summary":"**[ CVPR 2022 ]** GroupNet, a multiscale hypergraph neural network, which is novel in terms of both interaction capturing and representation learning.","tags":[],"title":"GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning","type":"project"},{"authors":["Chenxin Xu","Weibo Mao","Wenjun Zhang","Siheng Chen"],"categories":["CVPR 2022"],"content":"Abstract To realize trajectory prediction, most previous methods adopt the parameter-based approach, which encodes all the seen past-future instance pairs into model parameters. However, in this way, the model parameters come from all seen instances, which means a huge amount of irrelevant seen instances might also involve in predicting the current situation, disturbing the performance. To provide a more explicit link between the current situation and the seen instances, we imitate the mechanism of retrospective memory in neuropsychology and propose MemoNet, an instance-based approach that predicts the movement intentions of agents by looking for similar scenarios in the training data. In MemoNet, we design a pair of memory banks to explicitly store representative instances in the training set, acting as prefrontal cortex in the neural system, and a trainable memory addresser to adaptively search a current situation with similar instances in the memory bank, acting like basal ganglia. During prediction, MemoNet recalls previous memory by using the memory addresser to index related instances in the memory bank. We further propose a two-step trajectory prediction system, where the first step is to leverage MemoNet to predict the destination and the second step is to fulfill the whole trajectory according to the predicted destinations. Experiments show that the proposed MemoNet improves the FDE by 20.3%/10.2%/28.3% from the previous best method on SDD/ETH-UCY/NBA datasets. Experiments also show that our MemoNet has the ability to trace back to specific instances during prediction, promoting more interpretability.\nResult Diverse intention prediction by MemoNet on SDD, where 20 final intentions are clustered from 120 coarse intention anchors. MemoNet can provide diverse and accurate intention predictions.\nPrediction cases with corresponding past-future trajectories traced by memory addresser. MemoNet promotes a more explicit link between the current situation and seen instances.\nCitation @InProceedings{MemoNet_2022_CVPR, author = {Xu, Chenxin and Mao, Weibo and Zhang, Wenjun and Chen, Siheng}, title = {Remember Intentions: Retrospective-Memory-based Trajectory Prediction}, booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, year = {2022} } Acknowledgement This research is partially supported by the National Key R\u0026amp;D Program of China under Grant 2021ZD0112801, National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CCF-DiDi GAIA Research Collaboration Plan 202112.\n","date":1650281659,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650281659,"objectID":"c5b982c3e2bbed46e1587db167feca3c","permalink":"https://Siheng-Chen.github.io/project/memonet/","publishdate":"2022-04-18T19:34:19+08:00","relpermalink":"/project/memonet/","section":"project","summary":"**[ CVPR 2022 ]** MemoNet, an instance-based approach that predicts the movement intentions of agents by looking for similar scenarios in the training data.","tags":[],"title":"Remember Intentions: Retrospective-Memory-based Trajectory Prediction","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"https://Siheng-Chen.github.io/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/","section":"","summary":"","tags":null,"title":"Tour","type":"widget_page"}]