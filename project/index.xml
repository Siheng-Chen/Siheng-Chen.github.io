<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Siheng Chen</title><link>https://Siheng-Chen.github.io/project/</link><atom:link href="https://Siheng-Chen.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 23 Feb 2024 19:34:19 +0800</lastBuildDate><image><url>https://Siheng-Chen.github.io/media/icon_hufe4902d8a3e296f954ced894ecfc599d_303890_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://Siheng-Chen.github.io/project/</link></image><item><title>MATRIX: Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation</title><link>https://Siheng-Chen.github.io/project/matrix-simulation/</link><pubDate>Fri, 23 Feb 2024 19:34:19 +0800</pubDate><guid>https://Siheng-Chen.github.io/project/matrix-simulation/</guid><description>&lt;h1 id="matrix">MATRIX&lt;/h1>
&lt;p>MATRIX: Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./featured.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties’ concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user’s input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values.&lt;/p>
&lt;div style="text-align: center;">
&lt;img src="./images/abstract.png" width="70%" height="auto">
&lt;/div>
&lt;h2 id="matrix-framework">MATRIX Framework&lt;/h2>
&lt;p>MATRIX takes an instruction-response pair as input and outputs the social consequences behind an instruction. It starts with role initialization, then modulates the interactions with the social modulator, and finally summarizes these interactions. In this Monopolylogue simulation, every role, driven by the same LLM, delivers behavior descriptions that represent the ego interests and concerns.&lt;/p>
&lt;div style="text-align: center;">
&lt;img src="./images/framework.png" width="90%" height="auto">
&lt;/div>
&lt;h2 id="experimental-results">Experimental results&lt;/h2>
&lt;p>Pairwise comparisons between the LLM (30B) with MATRIX and 7 baselines. Win, Tie, Lose rates are reported with GPT-4 as the judger. The LLM with MATRIX consistently outperforms all of the baselines including GPT-3.5-Turbo on 4 evaluation datasets
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/exp1.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Human evaluation experiments. This study selected 100 questions from 14 harmful categories in the Safe-RLHF dataset for assessment. 875 human ratings indicated that the 13B LLM, fine-tuned on MATRIX, surpassed the answer quality of GPT-4 when facing harmful questions.&lt;/p>
&lt;div style="text-align: center;">
&lt;img src="./images/exp2.png" width="60%" height="auto">
&lt;/div>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript" data-lang="gdscript">&lt;span class="line">&lt;span class="cl">&lt;span class="err">@&lt;/span>&lt;span class="n">article&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">pang2024self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">title&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">Self&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">Alignment&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">Large&lt;/span> &lt;span class="n">Language&lt;/span> &lt;span class="n">Models&lt;/span> &lt;span class="n">via&lt;/span> &lt;span class="n">Monopolylogue&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">based&lt;/span> &lt;span class="n">Social&lt;/span> &lt;span class="n">Scene&lt;/span> &lt;span class="n">Simulation&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">author&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">Pang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Xianghe&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Tang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Shuo&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Ye&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Rui&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Xiong&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Yuxin&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Zhang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Bolun&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Wang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Yanfeng&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Chen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Siheng&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">journal&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">arXiv&lt;/span> &lt;span class="n">preprint&lt;/span> &lt;span class="n">arXiv&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">2402.05699&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">year&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">2024&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>CoAlign: Robust Collaborative 3D Object Detection in Presence of Pose Errors</title><link>https://Siheng-Chen.github.io/project/coalign/</link><pubDate>Fri, 20 Jan 2023 19:34:19 +0800</pubDate><guid>https://Siheng-Chen.github.io/project/coalign/</guid><description>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./featured.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Collaborative 3D object detection exploits information exchange among multiple agents to enhance accuracy of object detection in presence of sensor impairments such as occlusion. However, in practice, pose estimation errors due to imperfect localization would cause spatial message misalignment and significantly reduce the performance of collaboration. To alleviate adverse impacts of pose errors, we propose
CoAlign, a novel hybrid collaboration framework that is robust to unknown pose errors. The proposed solution relies on a novel agent-object pose graph modeling to enhance pose consistency among collaborating agents. Furthermore, we adopt a multi-scale data fusion strategy to aggregate intermediate features at multiple spatial resolutions. Comparing with previous works, which require ground-truth pose for training supervision, our proposed CoAlign is more practical since it doesn’t require any ground-truth pose supervision in the training and makes no specific assumptions on pose errors. Extensive evaluation of the proposed method is carried out on multiple datasets, certifying that CoAlign significantly reduce relative localization error and achieving the state of art detection performance when pose errors exist. Code are made available for the use of the research community at &lt;a href="https://github.com/yifanlu0227/CoAlign" target="_blank" rel="noopener">https://github.com/yifanlu0227/CoAlign&lt;/a>.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;h3 id="on-the-relational-reasoning">On the relational reasoning&lt;/h3>
&lt;p>Visualization of detected boxes in DAIR-V2X dataset. Green boxes are ground-truth while red ones are detection. CoAlign achieves much more precise detection.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/visualization.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Detection performance on OPV2V, V2X-Sim 2.0 and DAIR-V2X datasets with pose noises following Gaussian distribution in the testing phase. All models are trained on pose noises following Gaussian distribution with σt = 0.2m, σr = 0.2. Experiments show that CoAlign holds the best resistance to localization error under various noise levels.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/table.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript" data-lang="gdscript">&lt;span class="line">&lt;span class="cl">&lt;span class="err">@&lt;/span>&lt;span class="n">article&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">lu2022robust&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">title&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">Robust&lt;/span> &lt;span class="n">Collaborative&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="n">D&lt;/span> &lt;span class="nc">Object&lt;/span> &lt;span class="n">Detection&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">Presence&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">Pose&lt;/span> &lt;span class="n">Errors&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">author&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">Lu&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Yifan&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Li&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Quanhao&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Liu&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Baoan&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Dianati&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Mehrdad&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Feng&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Chen&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Chen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Siheng&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Wang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Yanfeng&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">journal&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">arXiv&lt;/span> &lt;span class="n">preprint&lt;/span> &lt;span class="n">arXiv&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">2211.07214&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">year&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">2022&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="acknowledgement">Acknowledgement&lt;/h2>
&lt;p>This research is partially supported by the National Key R&amp;amp;D Program of China under Grant 2021ZD0112801, National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CCF-DiDi GAIA Research Collaboration Plan 202112.&lt;/p></description></item><item><title>GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning</title><link>https://Siheng-Chen.github.io/project/groupnet/</link><pubDate>Mon, 18 Apr 2022 19:34:19 +0800</pubDate><guid>https://Siheng-Chen.github.io/project/groupnet/</guid><description>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/featured.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Demystifying the interactions among multiple agents from their past trajectories is fundamental to precise and interpretable trajectory prediction. However, previous works only consider pair-wise interactions with limited relational reasoning. To promote more comprehensive interaction modeling for relational reasoning, we propose GroupNet, a multiscale hypergraph neural network, which is novel in terms of both interaction capturing and representation learning. From the aspect of interaction capturing, we propose a trainable multiscale hypergraph to capture both pair-wise and group-wise interactions at multiple group sizes. From the aspect of interaction representation learning, we propose a three-element format that can be learnt end-to-end and explicitly reason some relational factors including the interaction strength and category. We apply GroupNet into both CVAE-based prediction system and previous state-of-the-art prediction systems for predicting socially plausible trajectories with relational reasoning. To validate the ability of relational reasoning, we experiment with synthetic physics simulations to reflect the ability to capture group behaviors, reason interaction strength and interaction category. To validate the effectiveness of prediction, we conduct extensive experiments on three real-world trajectory prediction datasets, including NBA, SDD and ETH-UCY; and we show that with GroupNet, the CVAE-based prediction system outperforms state-of-the-art methods. We also show that adding GroupNet will further improve the performance of previous state-of-the-art prediction systems.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;h3 id="on-the-relational-reasoning">On the relational reasoning&lt;/h3>
&lt;p>Visualization of learnt group behavior. (a) The particle trajectories containing a three-group with a light bar, a two-group with a spring and an individual particle. (b) The heatmap of the learnt affinity matrix via affinity modeling. (c) The multiscale hypergraph topology via hyperedge forming and the interaction category vector of each hyperedge. The red box represents the three-group and two-group we inferred.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/group_behavior.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Particles&amp;rsquo; trajectories and the curve of neural interaction strength with particle&amp;rsquo;s electric charge. We see that the neural interaction strength has a proportional relationship with the amount of charge, reflecting our model is capable to implicitly capture the interaction strength in an unsupervised manner.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/strength.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Visualization results on the NBA dataset. We plot the best trajectory among 20 predictions for the state-of-the-art method (NMMP), GroupNet with the CVAE framework (Ours) and ground truth (GT). The red/blue color represents players of two teams and the green color represents the basketball. Light color represents the past trajectory.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/nba.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript" data-lang="gdscript">&lt;span class="line">&lt;span class="cl">&lt;span class="err">@&lt;/span>&lt;span class="n">InProceedings&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">xu2022GroupNet&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">author&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">Xu&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Chenxin&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Li&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Maosen&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Ni&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Zhenyang&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Zhang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Ya&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Chen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Siheng&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">title&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">GroupNet&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Multiscale&lt;/span> &lt;span class="n">Hypergraph&lt;/span> &lt;span class="n">Neural&lt;/span> &lt;span class="n">Networks&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">Trajectory&lt;/span> &lt;span class="n">Prediction&lt;/span> &lt;span class="n">with&lt;/span> &lt;span class="n">Relational&lt;/span> &lt;span class="n">Reasoning&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">booktitle&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">The&lt;/span> &lt;span class="n">IEEE&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">CVF&lt;/span> &lt;span class="n">Conference&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="n">Computer&lt;/span> &lt;span class="n">Vision&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Pattern&lt;/span> &lt;span class="n">Recognition&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">CVPR&lt;/span>&lt;span class="p">)},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">year&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">2022&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="acknowledgement">Acknowledgement&lt;/h2>
&lt;p>This research is partially supported by the National Key R&amp;amp;D Program of China under Grant 2021ZD0112801, National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CCF-DiDi GAIA Research Collaboration Plan 202112.&lt;/p></description></item><item><title>Remember Intentions: Retrospective-Memory-based Trajectory Prediction</title><link>https://Siheng-Chen.github.io/project/memonet/</link><pubDate>Mon, 18 Apr 2022 19:34:19 +0800</pubDate><guid>https://Siheng-Chen.github.io/project/memonet/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>To realize trajectory prediction, most previous methods adopt the parameter-based approach, which encodes all the seen past-future instance pairs into model parameters. However, in this way, the model parameters come from all seen instances, which means a huge amount of irrelevant seen instances might also involve in predicting the current situation, disturbing the performance. To provide a more explicit link between the current situation and the seen instances, &lt;strong>we imitate the mechanism of retrospective memory in neuropsychology and propose MemoNet, an instance-based approach that predicts the movement intentions of agents by looking for similar scenarios in the training data&lt;/strong>. In MemoNet, we design a pair of memory banks to explicitly store representative instances in the training set, acting as prefrontal cortex in the neural system, and a trainable memory addresser to adaptively search a current situation with similar instances in the memory bank, acting like basal ganglia. During prediction, MemoNet recalls previous memory by using the memory addresser to index related instances in the memory bank. We further propose a two-step trajectory prediction system, where the first step is to leverage MemoNet to predict the destination and the second step is to fulfill the whole trajectory according to the predicted destinations. Experiments show that the proposed MemoNet improves the FDE by &lt;strong>20.3%/10.2%/28.3%&lt;/strong> from the previous best method on SDD/ETH-UCY/NBA datasets. Experiments also show that our MemoNet has the ability to trace back to specific instances during prediction, promoting more interpretability.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/MemoNet_predictions.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
Diverse intention prediction by MemoNet on SDD, where 20 final intentions are clustered from 120 coarse intention anchors. MemoNet can provide diverse and accurate intention predictions.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/MemoNet_result_interpretability.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
Prediction cases with corresponding past-future trajectories traced by memory addresser. MemoNet promotes a more explicit link between the current situation and seen instances.&lt;/p>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript" data-lang="gdscript">&lt;span class="line">&lt;span class="cl">&lt;span class="err">@&lt;/span>&lt;span class="n">InProceedings&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">MemoNet_2022_CVPR&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">author&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">Xu&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Chenxin&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Mao&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Weibo&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Zhang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Wenjun&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Chen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Siheng&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">title&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">Remember&lt;/span> &lt;span class="n">Intentions&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Retrospective&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">Memory&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">based&lt;/span> &lt;span class="n">Trajectory&lt;/span> &lt;span class="n">Prediction&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">booktitle&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">The&lt;/span> &lt;span class="n">IEEE&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">CVF&lt;/span> &lt;span class="n">Conference&lt;/span> &lt;span class="n">on&lt;/span> &lt;span class="n">Computer&lt;/span> &lt;span class="n">Vision&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Pattern&lt;/span> &lt;span class="n">Recognition&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">CVPR&lt;/span>&lt;span class="p">)},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">year&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">2022&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="acknowledgement">Acknowledgement&lt;/h2>
&lt;p>This research is partially supported by the National Key R&amp;amp;D Program of China under Grant 2021ZD0112801, National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CCF-DiDi GAIA Research Collaboration Plan 202112.&lt;/p></description></item></channel></rss>