<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: April 25, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Kaushan+Script&family=Martel+Sans&family=Orelega+One&family=Source+Serif+Pro:ital,wght@1,300&family=Roboto:wght@400;700&display=swap&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Kaushan+Script&family=Martel+Sans&family=Orelega+One&family=Source+Serif+Pro:ital,wght@1,300&family=Roboto:wght@400;700&display=swap&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.7c9112d62aa504adadca15bbc66e8e51.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Siheng Chen"><meta name=description content="Siheng Chen"><link rel=alternate hreflang=en-us href=https://Siheng-Chen.github.io/><link rel=canonical href=https://Siheng-Chen.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hufe4902d8a3e296f954ced894ecfc599d_303890_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hufe4902d8a3e296f954ced894ecfc599d_303890_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://Siheng-Chen.github.io/media/icon_hufe4902d8a3e296f954ced894ecfc599d_303890_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Siheng Chen"><meta property="og:url" content="https://Siheng-Chen.github.io/"><meta property="og:title" content="Siheng Chen"><meta property="og:description" content="Siheng Chen"><meta property="og:image" content="https://Siheng-Chen.github.io/media/icon_hufe4902d8a3e296f954ced894ecfc599d_303890_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-01-20T19:34:19+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://Siheng-Chen.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://Siheng-Chen.github.io/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Siheng Chen"><title>Siheng Chen</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Siheng Chen</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Siheng Chen</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/project><span>Projects</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Datasets</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://siheng-chen.github.io/dataset/coperception-uav/ data-target=https://siheng-chen.github.io/dataset/coperception-uav/><span>CoPerception-UAV</span></a>
<a class=dropdown-item href=https://siheng-chen.github.io/dataset/AM3D/ data-target=https://siheng-chen.github.io/dataset/AM3D/><span>AM3D</span></a>
<a class=dropdown-item href=https://siheng-chen.github.io/dataset/dair-v2x-c-complemented/ data-target=https://siheng-chen.github.io/dataset/dair-v2x-c-complemented/><span>DAIR-V2X-C-Complemented</span></a></div></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_huf813d250365c7dfc6749e3158f882118_8352_270x270_fill_q75_lanczos_center.jpg alt="Siheng Chen"><div class=portrait-title><h2>Siheng Chen</h2><h3>Associate Professor</h3><h3><a href=https://sjtu.edu.cn/ target=_blank rel=noopener><span>Shanghai Jiao Tong University</span></a></h3><h3><a href=https://cmic.sjtu.edu.cn/EN/Default.aspx target=_blank rel=noopener><span>Cooperative Medianet Innovation Center (CMIC)</span></a></h3><h3><a href=https://www.shlab.org.cn/ target=_blank rel=noopener><span>Shanghai Artificial Intelligence Laboratory</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:sihengc@sjtu.edu.cn aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/siheng-chen-705b2832/ target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=W_Q33RMAAAAJ&amp;hl=zh-CN" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li><li><a href=https://github.com/siheng-chen target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.zhihu.com/people/sjtu-magic target=_blank rel=noopener aria-label=zhihu><i class="fab fa-zhihu big-icon"></i></a></li><li><a href=/uploads/cv.pdf aria-label=cv><i class="ai ai-cv big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p>Siheng Chen 陈思衡 is a tenure-track associate professor of <a href=https://sjtu.edu.cn target=_blank rel=noopener>Shanghai Jiao Tong University</a> and co-PI at <a href=https://www.shlab.org.cn/ target=_blank rel=noopener>Shanghai AI laboratory</a>. He received his doctorate from Carnegie Mellon University. His research interests include graph machine learning and collective intelligence. Dr. Chen&rsquo;s work on sampling theory of graph data received the 2018 IEEE Signal Processing Society Young Author Best Paper Award. His co-authored paper on structural health monitoring received ASME SHM/NDE 2020 Best Journal Paper Runner-Up Award and another paper on 3D point cloud processing received the Best Student Paper Award at 2018 IEEE Global Conference on Signal and Information Processing. His technique on joint perception and prediction was applied on all the UBER&rsquo;s autonomous cars. Dr. Chen also contributed to the project of scene-aware interaction, winning MERL President&rsquo;s Award. He also serves as the associate editor of IEEE Transactions on Signal and Information Processing over Networks.</p><!-- 
  <i class="fas fa-download  pr-1 fa-fw"></i> Download my 

<a href="/cv.pdf" target="_blank">Curriculum vitae</a>
. --><h3 id=join-us>Join us:</h3><p>We are actively hiring! We are looking for motivated postdocs, interns, PhD/master/undergraduate students. If you are interested in working with us, please feel free to drop me an email!</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Graph machine learning</li><li>Multi-agent learning</li><li>Collaborative perception</li><li>Federated learning</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Ph.D in Electrical and Computer Engineering</p><p class=institution>Carnegie Mellon University</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Master of Science in Machine Learning</p><p class=institution>Carnegie Mellon University</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Bachelor of Science in Electronic Engineering</p><p class=institution>Beijing Institute of Technology</p></div></li></ul></div></div></div></div></div></section><section id=recent_news class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent News</h1><p class=mt-1><a href=/news>All news&#187;</a></p></div><div class="col-12 col-lg-8"><p><strong>[Apr. 2023]</strong> Two papers are accepted to The International Conference on Machine Learning (ICML)</p><p><strong>[Apr. 2023]</strong> <a href=https://github.com/MediaBrain-SJTU/CoCa3D target=_blank rel=noopener>CoCa3D</a> is reported by <a href=https://mp.weixin.qq.com/s/v9F1xSKeAl_95xght5HVNA target=_blank rel=noopener>机器之心</a></p><p><strong>[Mar. 2023]</strong> Four papers are accepted to The Conference on Computer Vision and Pattern Recognition (CVPR)</p><p><strong>[Jan. 2023]</strong> <a href=https://arxiv.org/abs/2211.07214 target=_blank rel=noopener>Robust collaborative 3D object detection in presence of pose errors</a> is accepted to 2023 IEEE International Conference on Robotics and Automation (ICRA)</p><p><strong>[Jan. 2023]</strong> <a href=https://arxiv.org/abs/2208.03974 target=_blank rel=noopener>Aerial Monocular 3D Object Detection</a> is accepted to IEEE Robotics and Automation Letters</p><p><strong>[Dec. 2022]</strong> <a href=https://ieeexplore.ieee.org/abstract/document/10017172/ target=_blank rel=noopener>Discriminative Radial Domain Adaptation</a> is accepted to IEEE Transactions on Image Processing</p><p><strong>[Nov. 2022]</strong> <a href=https://arxiv.org/abs/2202.08408 target=_blank rel=noopener>One paper</a> is accepted to IEEE Transactions on Knowledge and Data Engineering</p><p><strong>[Oct. 2022]</strong> One paper is accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence</p><p><strong>[Oct. 2022]</strong> <a href=https://github.com/MediaBrain-SJTU/where2comm target=_blank rel=noopener>Where2comm</a> is reported by <a href=https://www.jiqizhixin.com/articles/2022-10-11-28 target=_blank rel=noopener>机器之心</a></p><p><strong>[Sep. 2022]</strong> One paper is accepted to NeurIP 2022</p></div></div></div></section><section id=publications class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=./publication/>filtering publications</a>.</div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yifan Lu</span>, <span>Quanhao Li</span>, <span>Baoan Liu</span>, <span>Mehrdad Dianat</span>, <span>Chen Feng</span>, <span>Siheng Chen</span>, <span>Yanfeng Wang</span></span>
(2023).
<a href=/publication/lu-robust-2023/>Robust Collaborative 3D Object Detection in Presence of Pose Errors</a>.
<em>arXiv preprint arXiv:2211.07214</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/lu-robust-2023/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yue Hu</span>, <span>Shaoheng Fang</span>, <span>Weidi Xie</span>, <span>Siheng Chen</span></span>
(2023).
<a href=/publication/hu-aerial-2023/>Aerial monocular 3d object detection</a>.
<em>arXiv preprint arXiv:2208.03974</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/hu-aerial-2023/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zenan Huang</span>, <span>Jun Wen</span>, <span>Siheng Chen</span>, <span>Linchao Zhu</span>, <span>Nenggan Zheng</span></span>
(2023).
<a href=/publication/huang-discriminative-2023/>Discriminative Radial Domain Adaptation</a>.
<em>IEEE Transactions on Image Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/huang-discriminative-2023/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TIP.2023.3235583 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Ming Jin</span>, <span>Yu Zheng</span>, <span>Yuan-Fang Li</span>, <span>Siheng Chen</span>, <span>Bin Yang</span>, <span>Shirui Pan</span></span>
(2022).
<a href=/publication/jin-2022-multivariate/>Multivariate Time Series Forecasting with Dynamic Graph Neural ODEs</a>.
<em>IEEE Transactions on Knowledge and Data Engineering</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/jin-2022-multivariate/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yiqi Zhong</span>, <span>Zhenyang Ni</span>, <span>Siheng Chen</span>, <span>Ulrich Neumann</span></span>
(2022).
<a href=/publication/zhong-aware-2022/>Aware of the history: Trajectory forecasting with the local behavior data</a>.
<em>European Conference on Computer Vision (ECCV)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhong-aware-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zixing Lei</span>, <span>Shunli Ren</span>, <span>Yue Hu</span>, <span>Wenjun Zhang</span>, <span>Siheng Chen</span></span>
(2022).
<a href=/publication/lei-latency-aware-2022/>Latency-aware collaborative perception</a>.
<em>European Conference on Computer Vision (ECCV)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/lei-latency-aware-2022/cite.bib>Cite</a></p></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=projects class="home-section wg-portfolio"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Projects</h1></div><div class="col-12 col-lg-8"><span class="d-none default-project-filter">*</span><div class=project-toolbar><div class=project-filters><div class=btn-toolbar><div class="btn-group flex-wrap"><a href=# data-filter=* class="btn btn-primary btn-lg active">All</a>
<a href=# data-filter=.js-id-deep-learning class="btn btn-primary btn-lg">Deep Learning</a>
<a href=# data-filter=.js-id-demo class="btn btn-primary btn-lg">Other</a></div></div></div></div><div class="isotope projects-container row js-layout-row"><div class="col-12 isotope-item"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/project/coalign/>CoAlign: Robust Collaborative 3D Object Detection in Presence of Pose Errors</a></div><a href=/project/coalign/ class=summary-link><div class=article-style><strong>[ ICRA 2023 ]</strong><br>CoAlign, a novel hybrid collaboration framework that is robust to unknown pose errors.</div></a><div class="stream-meta article-metadata"><div><span>Yifan Lu</span>, <span>Quanhao Li</span>, <span>Baoan Liu</span>, <span>Mehrdad Dianati</span>, <span>Chen Feng</span>, <span>Siheng Chen</span>, <span>Yanfeng Wang</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2211.07214 target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yifanlu0227/CoAlign target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=/project/coalign/><img src=/project/coalign/featured_hu17c58409f4cbd3288ba2759e0de211b0_553252_150x0_resize_q75_h2_lanczos.webp height=128 width=150 alt="CoAlign: Robust Collaborative 3D Object Detection in Presence of Pose Errors" loading=lazy></a></div></div></div><div class="col-12 isotope-item"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/project/groupnet/>GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning</a></div><a href=/project/groupnet/ class=summary-link><div class=article-style><strong>[ CVPR 2022 ]</strong><br>GroupNet, a multiscale hypergraph neural network, which is novel in terms of both interaction capturing and representation learning.</div></a><div class="stream-meta article-metadata"><div><span>Chenxin Xu</span>, <span>Maosen Li</span>, <span>Zhenyang Ni</span>, <span>Ya Zhang</span>, <span>Siheng Chen</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2204.08770 target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/MediaBrain-SJTU/GroupNet target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.techbeat.net/talk-info?id=686" target=_blank rel=noopener>Talk</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zhuanlan.zhihu.com/p/510918682 target=_blank rel=noopener><i class="fab fa-zhihu mr-1"></i>Zhihu</a></div></div><div class=ml-3><a href=/project/groupnet/><img src=/project/groupnet/featured_hu3c049b915c211dea12e7ac161e36a7a8_670709_150x0_resize_q75_h2_lanczos.webp height=58 width=150 alt="GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning" loading=lazy></a></div></div></div><div class="col-12 isotope-item"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/project/memonet/>Remember Intentions: Retrospective-Memory-based Trajectory Prediction</a></div><a href=/project/memonet/ class=summary-link><div class=article-style><strong>[ CVPR 2022 ]</strong><br>MemoNet, an instance-based approach that predicts the movement intentions of agents by looking for similar scenarios in the training data.</div></a><div class="stream-meta article-metadata"><div><span>Chenxin Xu</span>, <span>Weibo Mao</span>, <span>Wenjun Zhang</span>, <span>Siheng Chen</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.11474 target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/MediaBrain-SJTU/MemoNet target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zhuanlan.zhihu.com/p/492362530 target=_blank rel=noopener><i class="fab fa-zhihu mr-1"></i>Zhihu</a></div></div><div class=ml-3><a href=/project/memonet/><img src=/project/memonet/featured_hud1a6dbd11ce411a92c7667f9199b7544_430662_150x0_resize_q75_h2_lanczos.webp height=46 width=150 alt="Remember Intentions: Retrospective-Memory-based Trajectory Prediction" loading=lazy></a></div></div></div></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Contact</h1></div><div class="col-12 col-lg-8"><ul class=fa-ul><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<span id=person-email><a href=mailto:sihengc@sjtu.edu.cn>sihengc@sjtu.edu.cn</a></span></li><li><i class="fa-li fas fa-map-marker fa-2x" aria-hidden=true></i>
<span id=person-address>800 Dongchuan Road, Shanghai, 200240</span></li><li><i class="fa-li fas fa-compass fa-2x" aria-hidden=true></i>
<span>Enter SEIEE Building 5 and take the stairs to Office 303A on Floor 3</span></li></ul></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by><p class="powered-by copyright-license-text">© 2023 Siheng-Chen</p></p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>