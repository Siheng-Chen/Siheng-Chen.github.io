<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ICRA 2023 | Siheng Chen</title><link>https://Siheng-Chen.github.io/category/icra-2023/</link><atom:link href="https://Siheng-Chen.github.io/category/icra-2023/index.xml" rel="self" type="application/rss+xml"/><description>ICRA 2023</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 20 Jan 2023 19:34:19 +0800</lastBuildDate><image><url>https://Siheng-Chen.github.io/media/icon_hufe4902d8a3e296f954ced894ecfc599d_303890_512x512_fill_lanczos_center_3.png</url><title>ICRA 2023</title><link>https://Siheng-Chen.github.io/category/icra-2023/</link></image><item><title>CoAlign: Robust Collaborative 3D Object Detection in Presence of Pose Errors</title><link>https://Siheng-Chen.github.io/project/coalign/</link><pubDate>Fri, 20 Jan 2023 19:34:19 +0800</pubDate><guid>https://Siheng-Chen.github.io/project/coalign/</guid><description>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./featured.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Collaborative 3D object detection exploits information exchange among multiple agents to enhance accuracy of object detection in presence of sensor impairments such as occlusion. However, in practice, pose estimation errors due to imperfect localization would cause spatial message misalignment and significantly reduce the performance of collaboration. To alleviate adverse impacts of pose errors, we propose
CoAlign, a novel hybrid collaboration framework that is robust to unknown pose errors. The proposed solution relies on a novel agent-object pose graph modeling to enhance pose consistency among collaborating agents. Furthermore, we adopt a multi-scale data fusion strategy to aggregate intermediate features at multiple spatial resolutions. Comparing with previous works, which require ground-truth pose for training supervision, our proposed CoAlign is more practical since it doesn’t require any ground-truth pose supervision in the training and makes no specific assumptions on pose errors. Extensive evaluation of the proposed method is carried out on multiple datasets, certifying that CoAlign significantly reduce relative localization error and achieving the state of art detection performance when pose errors exist. Code are made available for the use of the research community at &lt;a href="https://github.com/yifanlu0227/CoAlign" target="_blank" rel="noopener">https://github.com/yifanlu0227/CoAlign&lt;/a>.&lt;/p>
&lt;h2 id="result">Result&lt;/h2>
&lt;h3 id="on-the-relational-reasoning">On the relational reasoning&lt;/h3>
&lt;p>Visualization of detected boxes in DAIR-V2X dataset. Green boxes are ground-truth while red ones are detection. CoAlign achieves much more precise detection.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/visualization.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Detection performance on OPV2V, V2X-Sim 2.0 and DAIR-V2X datasets with pose noises following Gaussian distribution in the testing phase. All models are trained on pose noises following Gaussian distribution with σt = 0.2m, σr = 0.2. Experiments show that CoAlign holds the best resistance to localization error under various noise levels.
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./images/table.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="citation">Citation&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript" data-lang="gdscript">&lt;span class="line">&lt;span class="cl">&lt;span class="err">@&lt;/span>&lt;span class="n">article&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">lu2022robust&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">title&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">Robust&lt;/span> &lt;span class="n">Collaborative&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="n">D&lt;/span> &lt;span class="nc">Object&lt;/span> &lt;span class="n">Detection&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">Presence&lt;/span> &lt;span class="n">of&lt;/span> &lt;span class="n">Pose&lt;/span> &lt;span class="n">Errors&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">author&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">Lu&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Yifan&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Li&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Quanhao&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Liu&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Baoan&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Dianati&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Mehrdad&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Feng&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Chen&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Chen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Siheng&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">Wang&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Yanfeng&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">journal&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">arXiv&lt;/span> &lt;span class="n">preprint&lt;/span> &lt;span class="n">arXiv&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mf">2211.07214&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">year&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">2022&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="acknowledgement">Acknowledgement&lt;/h2>
&lt;p>This research is partially supported by the National Key R&amp;amp;D Program of China under Grant 2021ZD0112801, National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CCF-DiDi GAIA Research Collaboration Plan 202112.&lt;/p></description></item></channel></rss>